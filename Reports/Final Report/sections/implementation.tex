\section*{Implementation}

We selected the questions related to politics from the European Social Survey data. After filtering out the questions with categorical answer options, we were left with 41 questions. After manually reviewing these 41 questions with their answer options, we divided them into two groups: unipolar and bipolar. If the responses to a question indicated a degree of opinion, it was classified as unipolar, whereas questions that presented a spectrum between two opposing views were classified as bipolar. For instance, a question aimed at understanding trust in politicians was classified as unipolar, while questions like 'Should one be ashamed if a close family member is homosexual?'—where respondents could indicate agreement or disagreement with two opposing views—were classified as bipolar. Since the metadata of the survey questions was in HTML format, the BeautifulSoup library was used during preprocessing. In this phase, by parsing the metadata file, we extracted information about the countries included in the survey, the questions, and possible response options. These were then labeled as unipolar or bipolar according to the definitions above.

In the second phase, the data was preprocessed. We processed the ESS Round 11 data, which we received in CSV format, using Pandas. At this stage, a unique ID was created for each participant by combining country and idno, as idno was unique within each country but could be the same for different participants in other countries. Next, we established a standard convention for handling missing values. For some questions, responses could range from 1 to 10, with missing values indicated by 77, 88, and 99, while for other questions, missing values were indicated by 7, 8, and 9. For overall consistency, 7 was replaced with 77, 8 with 88, and 9 with 99. In the second step of data processing, normalization was performed based on whether the questions were unipolar or bipolar. For unipolar questions, the highest possible answer was rescaled to 1 and the lowest to 0. For bipolar questions, answers were rescaled between -1 and 1. To illustrate this rescaling, consider a unipolar question where participants indicate their level of trust in politicians on a scale from 0 to 10. If a response was 5, it would be normalized to 0.5; if this question had been bipolar, a value of 5 would correspond to 0. We performed this normalization to later determine the weights that each vector obtained from sentence transformers would hold in calculating participants' individual vectors for each question.

In the third phase, prompts to be provided to the large language model were prepared. We selected the most extreme response to each question as the pivot answer. We believed that choosing an extreme response would help the generated statement convey a clear stance, making it more informative. For example, for the question 'Do you trust politicians?' the pivot response was 'Complete Trust (10).' We used the ChatGPT-4o model to turn the questions and pivot responses into statements as if they were made by participants. Before deciding on ChatGPT-4o, we also considered other open-source language models; however, due to the inference time required for large models, the inconsistency in answers from smaller models, and the cost-effectiveness of the ChatGPT-4 model, we decided to use ChatGPT-4o. Additionally, we minimized bias and ensured that the questions and pivot responses were syntactically converted into statements. To maintain objectivity and consistency in the statements, we set the temperature parameter to 0. In addition to the pivot responses, we also created statements for each missing value. As a result, using ChatGPT-4o, we generated four statements for each question in the survey: pivot and missing values (refusal, don't know, no answer).

In the fourth phase, we use the BGE-M3 sentence transformer to obtain vectors for the statements generated in the third phase. This model allows us to map our sentences into mathematical space, where semantically similar sentences are positioned closer together, while semantically distant ones are farther apart. As a result of this phase, we obtain vector representations of the statements created using the pivot responses. These vectors will be used in the next phase to calculate the vectors that will represent the participants.

In the fifth phase, we calculate the vectors intended to represent participants' individual beliefs. To do this, we aggregated the vectors obtained in the previous phase by taking into account the participants' responses. During data preprocessing, participants' responses were normalized based on their position within the spectrum of possible answers. These normalized values were used as coefficients for the vectors representing each question, and the vectors were then summed to create a vector representing each participant. In this phase, we assumed that participants' responses reflect who they are and convey meaning regarding their overall value systems. For participants with missing values, vectors obtained from statements generated for missing values were included with a coefficient of 1.

We exclude the questions for which we want to predict participants' answers when calculating their individual belief vectors. This allows us to have a 'gold annotation' for the answers we aim to predict. The target questions we want to predict, like other questions, have their pivot statements and pivot vectors calculated, but they are not included in the calculation of vectors representing participants. Ultimately, we have a mathematical space containing both participants and target questions. To make predictions within this space, we project participants' vectors onto the vectors of the target questions. These projections create a distribution for each target question. In obtaining this distribution, we assume that the lengths of the projections relate to their proximity to the pivot responses of the target questions.

After obtaining a projection distribution for each target question, we trim the extreme ends of the distribution on both sides to identify participants at the extremes. We then predict the responses for these participants with the extreme values on the answer spectrum. For the remaining majority of the population, we divide the distribution into intervals based on the number of possible responses. We then predict participants' responses according to the interval in which they fall. For example, suppose that after excluding the extreme values, the lengths of the projection vectors range between 0.5 and 1, and our target question allows for values from 1 to 5. In this case, we divide the range from 0.5 to 1 into five intervals, predicting values as follows: those in the 0.5–0.6 interval are predicted as 1, those in the 0.6–0.7 interval as 2, and so forth, with values exceeding 1 being assigned to 5, representing the extreme.

One of the challenges we faced when predicting participants' responses and testing the accuracy of our predictions was that each question had a different response range and number of possible answers. Additionally, the distribution of participants' responses varied from question to question. The metric we chose needed to show how much better our predictions were than random guessing and take into account that our questions were on a Likert scale. To eliminate the random chance factor, we used Cohen's Kappa metric. Since our predictions were on a scale, there was a meaningful difference between predicting a 3 or 4 for a response that should be a 2. To account for this difference, we also used Spearman's correlation metric. We implemented these metrics using the sklearn library.