\section*{Introduction}
Large language models (LLMs) have made remarkable strides in recent years, particularly with the introduction of transformer-based architectures, which enabled the scaling of models from millions to trillions of parameters. This increase in size, along with the use of self-supervised learning techniques, has greatly enhanced their ability to perform a wide range of tasks, such as text generation, translation, and reasoning across multiple domains \cite{patil2024review, naveed2023comprehensive, kumar2024large}. The shift from task-specific to general-purpose models, as seen with GPT-4 and PaLM, has allowed for versatility and fine-tuning that can adapt to specialized tasks, leading to unprecedented performance across benchmarks and novel use cases \cite{hagos2024recent}. These advancements, while impressive, bring challenges such as scalability, computational demand, and ethical concerns, particularly regarding bias and resource efficiency \cite{hagos2024recent}.

With these advancement of large language models (LLMs) and their increasing ability to comprehend natural language, there has been a growing interest in leveraging unstructured data from the internet to extract public opinion \cite{dong2021review,bhatia2024advancing,zhang2024decoding,dui2024social}. In this context, social media provides an accessible platform, enabling large-scale studies \cite{Tumasjan2010PredictingEW,Stieglitz2012SocialMA}. Despite the increasing use of social media data (SMD) for public opinion analysis, traditional surveys maintain their status as one of the most reliable methods for measuring public sentiment. Research shows that while SMD offers timely insights, it is often limited by the non-representative nature of social media users \cite{Ruths2014SocialMedia} and the platform's possible tendency to amplify extreme or polarized opinions \cite{stray2023algorithmic, nordbrandt2023affective}. Additionally, the complexity of preprocessing and analyzing social media data introduces challenges in drawing clear, valid conclusions from these data sources. For example, constructing meaningful measures from SMD can be both time-consuming and susceptible to biases, as highlighted by studies in the literature \cite{reveilhac2022systematic}. In contrast, surveys provide a more structured and controlled method, ensuring diverse demographic representation, which makes them a more reliable tool for capturing the general population's attitudes. However, they come with their own limitations, such as respondents' reluctance to answer certain questions. Additionally, conducting a comprehensive survey may require posing a large number of questions, which is often impractical. As the number of questions increases, the participation rate tends to decrease \cite{galesic2009effects}, consequently undermining the generalizability of the results. Moreover, even if these challenges are addressed, the costs of conducting such surveys can become prohibitively high.

The challenges associated with the high costs of surveys and the difficulties in conducting reliable analyses with social media data may lead to the idea of using LLMs to replace survey participants \cite{sun2024random}. However, the potential biases inherent in LLMs and the ongoing debate about their ability to accurately reflect societal diversity suggest that we have not yet reached a point where this solution is feasible \cite{lee2024can, wang2024large,argyle2023out}.

For the reasons mentioned above, an alternative approach has emerged: using LLMs not to replace human participants or rely solely on raw social media data, but to enhance existing surveys \cite{kim2023ai}. According to this idea, LLMs could be used to predict participants' responses to unanswered questions, questions that were asked to some participants but not others, or even entirely unasked questions. \cite[Kim and Lee (2023)]{kim2023ai} have reported relative success in predictions for the first two types of questions, while predictions for unasked questions have not achieved the same level of accuracy. Furthermore, for prediction and evaluation, converting survey responses from scales like the Likert scale into binary categories, treating them as a two-class classification problem, has limitations. Let’s consider two different response distributions. For simplicity, let’s have four options that represent levels of agreement from 1 to 4. We have 100 participants. If we examine the participants' distribution as 25-25-25-25 versus 50-0-0-50 from the perspective of a binary classification problem, there is no difference. However, as can be seen, the first distribution exhibits a more uniform distribution, while the second distribution indicates a more polarized society. If we binarize the problem, we cannot see the nuances between these two distributions.

Predicting missing values has been investigated over the years using survey imputation methods. However, current machine learning-based survey imputation techniques often struggle with sparse data \cite{kim2023ai}. Additionally, merely relying on the statistical correlation between responses overlooks the semantic relationships between the questions themselves. Current deep learning-based imputation methods, while powerful, face significant limitations in terms of computational complexity and scalability. These models require substantial resources, making them impractical for large-scale or real-time applications. Performance inconsistencies arise across datasets with varying levels of missingness, as no single method consistently performs well in all scenarios. Moreover, many approaches focus solely on imputation accuracy, often neglecting the impact of imputed data on downstream tasks, which remains a critical area for improvement \cite{wang2024deep}. 

In this study, we aim to understand on whether leveraging semantic relationships among survey questions can help predict answers to unasked questions. To predict responses to unasked questions, we generate an embedding that captures the participants' opinions. This approach not only aims to predict their likely answers but also proposes participants' vector representations that we can use to measure the similarity between participants, enabling a range of computational social science applications. For example, in social network analysis, we can use vector representations to model the formation and dissolution of social ties by calculating the similarity of individuals based on their interaction patterns or shared attributes. This allows researchers to analyze how closely connected individuals are, predict future relationships, and identify influential nodes within a network \cite{xu2023studying}. In the context of social movements and online activism, these vectors can be used to identify groups with shared interests or ideologies by comparing their behavior or communication data, which helps track how movements grow, evolve, and influence public discourse \cite{amro2023integrated}. In polarization and echo chambers, vector representations can quantify how ideologically similar individuals are, offering insights into how polarized clusters form and interact over time. This allows researchers to model the evolution of polarized opinions and design interventions to mitigate echo chambers \cite{baumann2020modeling}. Lastly, in group dynamics and behavioral prediction, vectors enable the comparison of group members based on behavioral attributes, helping to predict how certain group compositions impact outcomes like collaboration success or conflict within teams. By modeling individual similarities, researchers can assess how different personality traits or skill sets affect group performance and outcomes \cite{kelly2006protest}.
