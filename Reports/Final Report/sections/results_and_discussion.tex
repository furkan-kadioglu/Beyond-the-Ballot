\section*{Results and Discussion}

Through our observations of the ESS data, we found that we could categorize questions into three different types. Accordingly, we selected one variable from each type as a target variable: unipolar (e.g., trstprl), yes/no (e.g., badge), and bipolar (e.g., lrscale). For the unipolar and bipolar variables, we chose those with the maximum number of response options to see how nuanced our observations were. Additionally, we made predictions for every variable in the dataset. While doing this, we excluded the target variable from the individual belief embedding calculation and used the remaining variables to obtain belief embeddings, which were then used for predicting the target variable for participants. Detailed results for each variable can be found in the appendix section.

The Mean Absolute metric yielded different values for the three different variables we evaluated. For the unipolar and bipolar variables, we observed similar values ranging between 0 and 5. When we investigated the reasons for such a wide range in this score, we found that the yes/no questions produced much lower values. We observed that this was because the limited number of options made it less likely to produce a significant absolute error. Following this, we decided to use Cohen's Kappa metric, which takes into account the randomness factor.

Cohen's Kappa is a metric aimed at eliminating the randomness factor, measuring the level of agreement between two annotators on a scale from 0 to 1. A value of 0 indicates almost no agreement. In our experiments with different variables from the three types, the Cohen's Kappa score yielded values very close to 0. This result did not change even when we selected different variables. Consequently, we removed participants with any missing values, reducing the size of our dataset. Our goal was to determine whether our method of including missing values in the individual belief embedding made a difference. However, since this also gave similar results, we concluded that the issue did not lie there. We then considered whether the low score could be due to a precision error. For example, when using Cohen's Kappa to evaluate a variable with a Likert scale ranging from 1 to 10, predicting a value of 3 instead of 2 or predicting a value of 9 was penalized equally. To better reflect the nuance between these two different cases in our assessment, we decided to use Spearman's correlation metric.

Spearman's correlation metric, similar to Cohen's Kappa, returns a result between 0 and 1, where a value close to 0 indicates almost no correlation. In our experiments using this metric, instead of seeing consistent results across all variables like with Cohen's Kappa, we observed values ranging from -0.2 to 0.47. Additionally, when we compared the average values obtained for unipolar variables with those for bipolar variables, we found that our proposed method performed relatively better for unipolar variables. We believe this difference in performance is due to how bipolar variables were included in the calculation of individual belief embeddings. When a participant's response to a bipolar variable was the opposite extreme rather than the pivot response, the vector representing that variable was included by multiplying by -1. The effectiveness of this method relied on the assumption that a statement with an opposite opinion would be represented in our semantic space by a vector of the same magnitude but in the opposite direction. The difference in performance between unipolar and bipolar variables suggests that this assumption may not hold true. We believe that achieving this could require fine-tuning the sentence transformer so that two opposing statements are represented as exact opposites under vector addition.

Additionally, when we examined the predicted values, we observed that a certain value was returned more frequently than others, and this was not specific to any variable; whether the variable was unipolar or bipolar did not affect this outcome. Upon analyzing the distribution of the predicted values, we found that a distribution similar to a normal distribution repeated across all variables. While the extent to which this distribution was concentrated or spread out varied between variables, they all resembled the same distribution. However, when we examined the distribution of actual responses given by participants, we could observe more diverse distributions among the options. Due to this type of distribution, our predictions tended to yield relatively high Spearman correlation scores when they clustered around the most popular response in the survey, but when they failed to capture this, the scores approached 0. We believe that this distribution explains why the Spearman correlation data had high variance across variables.

In conclusion, it is evident that the extent to which the vectors representing participants in the generated space truly represent them remains open to further investigation. The distribution created using the length of the projection onto the target variable was a naive approach for predicting responses to the target question. Additionally, we believe that fine-tuning the sentence transformer used to create this space is necessary to close the performance gap between bipolar and unipolar variables.
 