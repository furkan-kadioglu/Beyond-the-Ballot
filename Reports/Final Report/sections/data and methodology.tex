\section*{Data and Methodology}

\subsection*{Dataset}

The dataset used in this study comes from the European Social Survey (ESS) Round 11, which includes standard political variables collected across 13 European countries with a total of 22,190 participants. The chosen variables focus on political engagement, attitudes, and trust in political institutions. Examples of the variables include political interest (polintr), trust in parliament (trstprl), and placement on the left-right political scale (lrscale). These variables were selected to explore how individuals' political attitudes and trust levels differ across countries and to examine correlations between political participation and socio-political beliefs. For representative data collection, the ESS data follows standard practices for survey sampling, including the use of inclusion probabilities, clustering, stratification, and the calculation of design weights and design effects \cite{jowell2007measuring, lynn2004methods}, making it a suitable choice for public opinion analysis.

\subsection*{Methodology}

First, questions from the ESS dataset that could be answered using a specific scale were selected, and questions like "Which party did you vote for?" that could not be scaled were excluded. The analysis included questions grouped into three categories. The first category comprised questions that could be answered on a bipolar scale ranging from "strongly disagree" (extremely dissatisfied) to "strongly agree" (extremely satisfied), with responses normalized to a range between -1 and 1. The second category involved questions with a unipolar scale, such as the level of trust in politicians, and responses were normalized between 0 and 1. Lastly, yes/no questions, which do not have intermediate values, were normalized similarly to questions on a bipolar scale.

In the second phase, the questions and their corresponding responses were transformed into statements using a large language model (LLM), remaining as close to the original syntax as possible to prevent additional bias. For example, the question \textit{How able do you think you are to take an active role in a group involved with political issues?} and its pivot response \textit{Completely able} were converted into the statement, \textit{I believe I am completely able to take an active role in a group involved with political issues.} Our goal at this stage was not to generate a statement for every possible question and answer pair, but rather to produce statements for the pivot responses (e.g., 5-\textit{completely able} on a scale from 1 to 5). These pivot statements were used to produce the pivot vectors in the third stage. Additionally, we had three types of missing values in our dataset, and one statement was produced for each of these missing values. In total, for each question, we obtained four question-answer pairs for the \textit{pivot response}, \textit{Don't know}, \textit{Refusal}, \textit{No answer}. Then, four statements were generated to produce a vector for each pair.

In the third phase, we collected the semantic representations of the statements created in the previous phase. To achieve this, we utilized a sentence transformer \cite[\textit{BGE-M3}]{chen2024bge} to derive the mathematical representations of the statements, as it is one of the top performers in the Massive Text Embedding\footnote{\url{https://huggingface.co/spaces/mteb/leaderboard}} and Thai Sentence Embedding\footnote{\url{https://huggingface.co/spaces/panuthept/thai_sentence_embedding_benchmark}} benchmarks as of October 2, 2024. Its open-source nature and availability in the Hugging Face library also facilitated straightforward implementation. Then, these vector representations are aggregated to create an individual belief embedding for each participant. The responses normalized in the first phase are used as coefficients for the vector representations of the pivot statements. If the participant refused to answer, selected "don't know," or didn't have an answer, the embedding of the corresponding statement was used. For example, in the case where the participant had options ranging from 1 (Not at all able) to 5 (Completely able), the pivot statement generated in the second phase from the pivot response \textit{completely able} was used to obtain its pivot vector representation. If the participant selected option 3 (Quite able), the normalized coefficient of 0.5 is applied to the corresponding statement's vector representation. If the participant refused to answer, the vector representation of the statement generated for the refusal option is included with a coefficient of 1. Finally, the participant's individual belief embedding is obtained by summing these vectors with the corresponding coefficients.

Finally, after obtaining the vector representations that reflect each user's individual beliefs based on their stated opinions, we predicted the participants' answers that would be given to questions that were not included in their individual belief embeddings. Pivot statements were generated for these excluded questions, and their vector representations were obtained. To predict the answers to these excluded questions, the projection of the participants' belief vectors onto the target question's vector was calculated. The distribution derived from these projections was used to estimate the participants' responses to these omitted questions. 
